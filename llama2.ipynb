{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/glm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-11 13:27:37,762] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:08<00:00, 22.81s/it]\n",
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "repo_id = \"/root/share/LLama2/llama-2-13b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(repo_id, device_map='auto', torch_dtype=torch.float16, load_in_8bit=False)\n",
    "model = model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id, use_fast=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Human: Introduce the history of Shenzhen\n",
      "</s><s> Assistant:  Sure, I'd be happy to help! Here is a brief overview of the history of Shenzhen:\n",
      "\n",
      "Shenzhen has been inhabited for thousands of years and was once known as \"Binhai\" or \"Fisher Girl's Head.\" In the early 1980s, it was designated as China's first special economic zone (SEZ) and underwent rapid development. Today, Shenzhen is one of the most prosperous cities in China and is home to many major companies such as Huawei Technologies Co., Ltd. and Tencent Holdings Limited. The city also boasts numerous universities and research institutions, including Tsinghua University-Shenzhen Graduate School and the Chinese Academy of Sciences. Additionally, Shenzhen is famous for its vibrant culture scene with various museums like OCT-LOFT Creative Culture Park and Window of the World theme park which showcases miniature versions of world landmarks. Would you like me to elaborate on any specific aspect?  </s>\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "input_ids = tokenizer(['<s>Human: Introduce the history of Shenzhen\\n</s><s>Assistant: '], return_tensors=\"pt\",add_special_tokens=False).input_ids.to('cuda')        \n",
    "generate_input = {\n",
    "    \"input_ids\":input_ids,\n",
    "    \"max_new_tokens\":512,\n",
    "    \"do_sample\":True,\n",
    "    \"top_k\":50,\n",
    "    \"top_p\":0.95,\n",
    "    \"temperature\":0.3,\n",
    "    \"repetition_penalty\":1.3,\n",
    "    \"eos_token_id\":tokenizer.eos_token_id,\n",
    "    \"bos_token_id\":tokenizer.bos_token_id,\n",
    "    \"pad_token_id\":tokenizer.pad_token_id\n",
    "}\n",
    "generate_ids  = model.generate(**generate_input)\n",
    "text = tokenizer.decode(generate_ids[0])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">root</span>\n",
       "├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">model </span><span style=\"color: #008000; text-decoration-color: #008000\">(LlamaModel)</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">embed_tokens </span><span style=\"color: #008000; text-decoration-color: #008000\">(Embedding) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[32000, 5120]</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layers </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleList)</span>\n",
       "│   │   └── <span style=\"color: #800000; text-decoration-color: #800000\">0-39</span><span style=\"color: #008000; text-decoration-color: #008000\">(LlamaDecoderLayer)</span>\n",
       "│   │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">self_attn </span><span style=\"color: #008000; text-decoration-color: #008000\">(LlamaAttention)</span>\n",
       "│   │       │   └── <span style=\"color: #800000; text-decoration-color: #800000\">q_proj,k_proj,v_proj,o_proj</span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[5120, 5120]</span>\n",
       "│   │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">mlp </span><span style=\"color: #008000; text-decoration-color: #008000\">(LlamaMLP)</span>\n",
       "│   │       │   ├── <span style=\"color: #800000; text-decoration-color: #800000\">gate_proj,up_proj</span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[13824, 5120]</span>\n",
       "│   │       │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">down_proj </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[5120, 13824]</span>\n",
       "│   │       └── <span style=\"color: #800000; text-decoration-color: #800000\">input_layernorm,post_attention_layernorm</span><span style=\"color: #008000; text-decoration-color: #008000\">(LlamaRMSNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[5120]</span>\n",
       "│   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(LlamaRMSNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[5120]</span>\n",
       "└── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">lm_head </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[32000, 5120]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mroot\u001b[0m\n",
       "├── \u001b[37mmodel \u001b[0m\u001b[32m(LlamaModel)\u001b[0m\n",
       "│   ├── \u001b[37membed_tokens \u001b[0m\u001b[32m(Embedding) \u001b[0m\u001b[36mweight:[32000, 5120]\u001b[0m\n",
       "│   ├── \u001b[37mlayers \u001b[0m\u001b[32m(ModuleList)\u001b[0m\n",
       "│   │   └── \u001b[31m0-39\u001b[0m\u001b[32m(LlamaDecoderLayer)\u001b[0m\n",
       "│   │       ├── \u001b[37mself_attn \u001b[0m\u001b[32m(LlamaAttention)\u001b[0m\n",
       "│   │       │   └── \u001b[31mq_proj,k_proj,v_proj,o_proj\u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[5120, 5120]\u001b[0m\n",
       "│   │       ├── \u001b[37mmlp \u001b[0m\u001b[32m(LlamaMLP)\u001b[0m\n",
       "│   │       │   ├── \u001b[31mgate_proj,up_proj\u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[13824, 5120]\u001b[0m\n",
       "│   │       │   └── \u001b[37mdown_proj \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[5120, 13824]\u001b[0m\n",
       "│   │       └── \u001b[31minput_layernorm,post_attention_layernorm\u001b[0m\u001b[32m(LlamaRMSNorm) \u001b[0m\u001b[36mweight:[5120]\u001b[0m\n",
       "│   └── \u001b[37mnorm \u001b[0m\u001b[32m(LlamaRMSNorm) \u001b[0m\u001b[36mweight:[5120]\u001b[0m\n",
       "└── \u001b[37mlm_head \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[32000, 5120]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bigmodelvis import Visualization\n",
    "\n",
    "Visualization(model).structure_graph();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
