{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-11 16:17:49,748] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:52<00:00, 17.59s/it]\n",
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "repo_id = \"./ckpt/llama-2-13b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(repo_id, device_map='auto', torch_dtype=torch.float16, load_in_8bit=False)\n",
    "model = model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id, use_fast=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Human: Introduce the history of Shenzhen\n",
      "</s><s> Assistant:  Sure, I'd be happy to help! Here is a brief overview of the history of Shenzhen:\n",
      "\n",
      "Shenzhen has undergone one of the most rapid transformations in modern Chinese history. Prior to 1978, it was just a small fishing village with less than 20,000 residents. However, after China began its economic reforms and opened up to foreign investment, Shenzhen became one of the first Special Economic Zones (SEZs) established by the government. This designation allowed for greater autonomy from central planning and regulations, which attracted many domestic and international businesses looking to take advantage of cheap labor costs and favorable policies. As a result, Shenzhen experienced an influx of migrant workers who came seeking better job opportunities; today there are now more than 25 million people living within city limits making it one of fastest growing cities globally not only economically but also demographically speaking too as well! With this growth came significant infrastructure development such as roads highways airports seaports telecommunications networks etc., all designed towards facilitating trade commerce industry & tourism alike leading further prosperity & progress throughout region . Today , Shenzen continues grow rapidly becoming major hub technology innovation finance manufacturing logistics serving entire Asia-Pacific region while maintaining strong ties global markets worldwide ensuring continued success story will continue unfold long into future ! Is that what you were hoping for? Do you have any specific questions about Shenzhen or its history ? </s>\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "input_ids = tokenizer(['<s>Human: Introduce the history of Shenzhen\\n</s><s>Assistant: '], return_tensors=\"pt\",add_special_tokens=False).input_ids.to('cuda')        \n",
    "generate_input = {\n",
    "    \"input_ids\":input_ids,\n",
    "    \"max_new_tokens\":512,\n",
    "    \"do_sample\":True,\n",
    "    \"top_k\":50,\n",
    "    \"top_p\":0.95,\n",
    "    \"temperature\":0.3,\n",
    "    \"repetition_penalty\":1.3,\n",
    "    \"eos_token_id\":tokenizer.eos_token_id,\n",
    "    \"bos_token_id\":tokenizer.bos_token_id,\n",
    "    \"pad_token_id\":tokenizer.pad_token_id\n",
    "}\n",
    "generate_ids  = model.generate(**generate_input)\n",
    "text = tokenizer.decode(generate_ids[0])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">root</span>\n",
       "├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">model </span><span style=\"color: #008000; text-decoration-color: #008000\">(LlamaModel)</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">embed_tokens </span><span style=\"color: #008000; text-decoration-color: #008000\">(Embedding) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[32000, 5120]</span>\n",
       "│   ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">layers </span><span style=\"color: #008000; text-decoration-color: #008000\">(ModuleList)</span>\n",
       "│   │   └── <span style=\"color: #800000; text-decoration-color: #800000\">0-39</span><span style=\"color: #008000; text-decoration-color: #008000\">(LlamaDecoderLayer)</span>\n",
       "│   │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">self_attn </span><span style=\"color: #008000; text-decoration-color: #008000\">(LlamaAttention)</span>\n",
       "│   │       │   └── <span style=\"color: #800000; text-decoration-color: #800000\">q_proj,k_proj,v_proj,o_proj</span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[5120, 5120]</span>\n",
       "│   │       ├── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">mlp </span><span style=\"color: #008000; text-decoration-color: #008000\">(LlamaMLP)</span>\n",
       "│   │       │   ├── <span style=\"color: #800000; text-decoration-color: #800000\">gate_proj,up_proj</span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[13824, 5120]</span>\n",
       "│   │       │   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">down_proj </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[5120, 13824]</span>\n",
       "│   │       └── <span style=\"color: #800000; text-decoration-color: #800000\">input_layernorm,post_attention_layernorm</span><span style=\"color: #008000; text-decoration-color: #008000\">(LlamaRMSNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[5120]</span>\n",
       "│   └── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">norm </span><span style=\"color: #008000; text-decoration-color: #008000\">(LlamaRMSNorm) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[5120]</span>\n",
       "└── <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">lm_head </span><span style=\"color: #008000; text-decoration-color: #008000\">(Linear) </span><span style=\"color: #008080; text-decoration-color: #008080\">weight:[32000, 5120]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mroot\u001b[0m\n",
       "├── \u001b[37mmodel \u001b[0m\u001b[32m(LlamaModel)\u001b[0m\n",
       "│   ├── \u001b[37membed_tokens \u001b[0m\u001b[32m(Embedding) \u001b[0m\u001b[36mweight:[32000, 5120]\u001b[0m\n",
       "│   ├── \u001b[37mlayers \u001b[0m\u001b[32m(ModuleList)\u001b[0m\n",
       "│   │   └── \u001b[31m0-39\u001b[0m\u001b[32m(LlamaDecoderLayer)\u001b[0m\n",
       "│   │       ├── \u001b[37mself_attn \u001b[0m\u001b[32m(LlamaAttention)\u001b[0m\n",
       "│   │       │   └── \u001b[31mq_proj,k_proj,v_proj,o_proj\u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[5120, 5120]\u001b[0m\n",
       "│   │       ├── \u001b[37mmlp \u001b[0m\u001b[32m(LlamaMLP)\u001b[0m\n",
       "│   │       │   ├── \u001b[31mgate_proj,up_proj\u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[13824, 5120]\u001b[0m\n",
       "│   │       │   └── \u001b[37mdown_proj \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[5120, 13824]\u001b[0m\n",
       "│   │       └── \u001b[31minput_layernorm,post_attention_layernorm\u001b[0m\u001b[32m(LlamaRMSNorm) \u001b[0m\u001b[36mweight:[5120]\u001b[0m\n",
       "│   └── \u001b[37mnorm \u001b[0m\u001b[32m(LlamaRMSNorm) \u001b[0m\u001b[36mweight:[5120]\u001b[0m\n",
       "└── \u001b[37mlm_head \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[36mweight:[32000, 5120]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bigmodelvis import Visualization\n",
    "\n",
    "Visualization(model).structure_graph();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
